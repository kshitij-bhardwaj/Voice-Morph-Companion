{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6toho7uRO7v"
      },
      "source": [
        "# Question 1\n",
        "## Developing an Artificial Neural Network from Scratch.\n",
        "\n",
        "In this notebook, we will be developing a fully connected feedforward neural network.\n",
        "\n",
        "We will import the MNIST dataset from keras datsets. The MNIST dataset contains images of 28x28 pixels each having values ranging from 0-255.\n",
        "It has 60000 images in the training set and 10000 images in the test set. However, we will only use the first 10000 images for training and first 1000 images for testing because our code isn't optimized and it takes time to run. We are not looking for accuracy of our network right now, we will be doing that in the next question when we will be implementing the same using Tensorflow.\n",
        "\n",
        "\n",
        "Run the first 3 cells. Your code begins after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI17X78rktdA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrINntzulT4M",
        "outputId": "72879853-cec4-4d1e-dd27-245e969cd0b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr4rLzb9ZBQE"
      },
      "source": [
        "As discussed in the class, the images are flattened to a column.\n",
        "\n",
        "Then we are normalizing them by dividing by 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jy7CLWCEwfn"
      },
      "outputs": [],
      "source": [
        "train_X=train_X.reshape(60000,784,1)    # flattening\n",
        "test_X=test_X.reshape(10000,784,1)\n",
        "\n",
        "train_y=train_y.reshape(60000,1)\n",
        "test_y=test_y.reshape(10000,1)\n",
        "\n",
        "train_X= train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "train_X=train_X[:10000]         #taking the first 10000 images.\n",
        "train_y=train_y[:10000]\n",
        "test_X=test_X[:1000]\n",
        "test_y=test_y[:1000]\n",
        "train_data=list(zip(train_X,train_y))\n",
        "test_data=list(zip(test_X,test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWwDzh6kZOy3"
      },
      "source": [
        "## 1.1 Write the code for Sigmoid Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q5a8tGYku-7"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return \"\"\"....\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIJI5SoxbJaq"
      },
      "source": [
        "## 1.2 The Network\n",
        "\n",
        "We will making a class called Network which has certain functions inside it. The cost function used is Cross-Entropy Loss. You need to code only the first 3. Rest are done for you.  There are various places within the code marked as stop_zone. Read the instructions below the code at those places to check whether your code till there is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwsydmyTEt0z"
      },
      "outputs": [],
      "source": [
        "class Network(object):\n",
        "    def __init__(self,sizes): # sizes is a list containing the network.\n",
        "                              # eg : [784,128,10] means input =784 neurons,\n",
        "                              #    1st hidden layer 128 neurons, output 10 neurons.\n",
        "        self.sizes=sizes\n",
        "        self.num_layers=\"...can you say the number of layers based on the list called sizes...\"\n",
        "        self.weights= [np.random.randn(x,y) for x,y in zip(sizes[1:],sizes[:-1])]\n",
        "        self.biases= \"...can you do this by understanding the self.weights...\"\n",
        "\n",
        "# stop_zone 1. Comment out all the code below. Select all rows below. Click Ctrl + /.\n",
        "# Include the show function given below above this comment area inside the class.\n",
        "# Run this cell and then run the code with stop_zone 1 written below.\n",
        "# After this testing, don't forget to remove the comments. Same, select all, Ctrl+/.\n",
        "\n",
        "    def forwardpropagation(self,a):\n",
        "        for b,w in zip(self.biases, self.weights):\n",
        "            a=\"...a is activation...\" # sig (w.a +b)\n",
        "            # print(a.shape)\n",
        "        return a\n",
        "\n",
        "# stop_zone 2. Comment out all the code below. Don't comment out the __init__ method else you will get error.\n",
        "# Remove comment from print(a.shape) line above. Run this cell. And run the code with stop_zone 2 written below.\n",
        "\n",
        "\n",
        "    def backpropagation(self,x,y):\n",
        "\n",
        "        # nothing to do in this 3 lines. it is for creating a one-hot encoded vector of the labels.\n",
        "        y_t = np.zeros((len(y), 10))\n",
        "        y_t[np.arange(len(y)), y] = 1\n",
        "        y_t= y_t.T\n",
        "\n",
        "        #nabla_b=dC/db and nabla_w=dC/dw. They are lists of shapes equal to that of bias and weights.\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # initially, a0 = input.\n",
        "        activation=x\n",
        "        activation_list=[x]\n",
        "\n",
        "        # step 1 : calculation of delta in last layer\n",
        "\n",
        "        # write the same forward propagation code here but while doing so store the a's.\n",
        "        for w,b in zip(self.weights,self.biases):\n",
        "            activation= \"...just written above...\"\n",
        "            activation_list.append(activation)\n",
        "\n",
        "        delta= \"...delta is dC/dz3... how is it calculated?...\"\n",
        "\n",
        "        # step 2 : nabla_b and nabla_w relation with delta of last layer\n",
        "\n",
        "        nabla_b[-1]=\"...how is dC/db3 and dC/dz3 related...\"\n",
        "        nabla_w[-1]= \"...how is dC/dw3 and dC/dz3 related...\"\n",
        "\n",
        "        # print(\"{} {}\".format(nabla_b[-1].shape,nabla_w[-1].shape) )\n",
        "#stop_zone 3 : remove comment from the print statement just above and run the cell for stop_zone3.\n",
        "# don't forget commenting out.\n",
        "\n",
        "        # step 3 : calculation of delta for hidden layers\n",
        "\n",
        "        for j in range(2,self.num_layers):\n",
        "            sig_der = activation_list[-j]*(1-activation_list[-j])\n",
        "            delta= \"...how is dC/dz2 and dC/dz3 related ? Look i have calculated one term already for you (sig_der)...\"\n",
        "\n",
        "            # step 4 : nabla_b and nabla_w relation with delta of others layers\n",
        "            nabla_b[-j]= \"...again, how is dC/db2 and dC/dz2 related...\"\n",
        "            nabla_w[-j]= \"...how is dC/dw2 and dC/dz2 related...\"\n",
        "\n",
        "        return (nabla_b,nabla_w)\n",
        "#stop_zone 4 : Run the cell for stop_zone 4.\n",
        "\n",
        "    def SGD(self, train_data,epochs,mini_batch_size, lr):\n",
        "        n_train= len(train_data)\n",
        "        for i in range(epochs):\n",
        "            random.shuffle(train_data)\n",
        "            mini_batches = [\"\"\"...Split the data into batches each of size = mini_batch_size  ...\"\"\"]\n",
        "\n",
        "  # Stop zone 5 : Remove comment from the next print line and comment out all the lines below it.\n",
        "        # print(np.array(mini_batches, dtype=object).shape)\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch,lr)\n",
        "\n",
        "            self.predict(train_data)\n",
        "            print(\"Epoch {0} completed.\".format(i+1))\n",
        "\n",
        "    # the functions below are complete. If you are fine till stop_zone 5, you can run\n",
        "    # this whole cell and train, test the data by running the last cell of the notebook.\n",
        "    # You may need to wait for around 10 minutes to see the test predictions.\n",
        "\n",
        "    def update_mini_batch(self,mini_batch,lr):\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "        for x,y in mini_batch:\n",
        "            delta_b,delta_w= self.backpropagation(x,y)\n",
        "            nabla_b=[nb+ db for nb,db in zip (nabla_b,delta_b)]\n",
        "            nabla_w=[nw+dw for nw,dw in zip(nabla_w,delta_w)]\n",
        "\n",
        "        self.weights=[w- lr*nw/len(mini_batch) for w,nw in zip(self.weights,nabla_w)]\n",
        "        self.biases=[b-lr*nb/len(mini_batch) for b,nb in zip(self.biases,nabla_b)]\n",
        "\n",
        "    def predict(self,test_data):\n",
        "        test_results = [(np.argmax(self.forwardpropagation(x)),y) for x,y in test_data]\n",
        "        # returns the index of that output neuron which has highest activation\n",
        "\n",
        "        num= sum(int (x==y) for x,y in test_results)\n",
        "        print (\"{0}/{1} classified correctly.\".format(num,len(test_data)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u8cVnGamVgP"
      },
      "outputs": [],
      "source": [
        "# stop_zone 1\n",
        "\n",
        "def show(self):\n",
        "  print(self.num_layers)\n",
        "  for bias in self.biases:\n",
        "      print(bias.shape)\n",
        "  for weight in self.weights:\n",
        "      print(weight.shape)\n",
        "\n",
        "# Copy this show function from here. Paste it inside that Network Class.\n",
        "# Comment out the show function here. Run this cell.\n",
        "\n",
        "net=Network([784,128,64,10])\n",
        "net.show()\n",
        "\n",
        "# The desired output is :\n",
        "# 4\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation.\n",
        "\n",
        "# Keeping the show function over there in the Network class doesn't make any\n",
        "# difference. You may delete it if you wish. Better toss a coin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7EJBF7XsSft"
      },
      "outputs": [],
      "source": [
        "# stop_zone 2\n",
        "# to use this, make sure your data is loaded. Run this cell.\n",
        "net=Network([784,128,64,10])\n",
        "print(train_X[0])\n",
        "net.forwardpropagation(train_X[0])\n",
        "\n",
        "# The desired output is :\n",
        "# (784, 1)\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwHWyaKNhIIk"
      },
      "outputs": [],
      "source": [
        "# stop_zone 3\n",
        "net=Network([784,128,64,10])\n",
        "net.backpropagation(train_X[0],train_y[0])\n",
        "\n",
        "# Desired output : (10,1) (10,64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pq4E3rHik-f"
      },
      "outputs": [],
      "source": [
        "# stop_zone 4\n",
        "net=Network([784,128,64,10])\n",
        "nabla_b,nabla_w=net.backpropagation(train_X[0],train_y[0])\n",
        "for nb in nabla_b:\n",
        "  print(nb.shape)\n",
        "for nw in nabla_w:\n",
        "  print(nw.shape)\n",
        "\n",
        "# Desired output:\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaCblO7XE0Fy"
      },
      "outputs": [],
      "source": [
        "# Stop zone 5 :  Run this cell, for 10000 samples and batch size of 20, output should be\n",
        "#       (500,20,2).  500 batches each of size 20 and has 2 objects : train and test data.\n",
        "\n",
        "net=Network([784,256,128,64,10])\n",
        "net.SGD(train_data=train_data,epochs=20,mini_batch_size=20,lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXljiAYRlvdq"
      },
      "outputs": [],
      "source": [
        "net=Network([784,128,64,10])\n",
        "net.SGD(train_data=train_data,epochs=10,mini_batch_size=20,lr=0.01)\n",
        "print(\"Test data:\")\n",
        "net.predict(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhMIoFT9m7OU"
      },
      "source": [
        "# End of question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 2\n",
        "\n",
        "In this question, you'll build a model to predict hotel cancellations with a binary classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.available\n",
        "# Set Matplotlib defaults\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc('axes', labelweight='bold', labelsize='large',\n",
        "       titleweight='bold', titlesize=18, titlepad=10)\n",
        "plt.rc('animation', html='html5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "hotel = pd.read_csv('hotel.csv')\n",
        "\n",
        "X = hotel.copy()\n",
        "y = X.pop('is_canceled')\n",
        "\n",
        "X['arrival_date_month'] = \\\n",
        "    X['arrival_date_month'].map(\n",
        "        {'January':1, 'February': 2, 'March':3,\n",
        "         'April':4, 'May':5, 'June':6, 'July':7,\n",
        "         'August':8, 'September':9, 'October':10,\n",
        "         'November':11, 'December':12}\n",
        "    )\n",
        "\n",
        "features_num = [\n",
        "    \"lead_time\", \"arrival_date_week_number\",\n",
        "    \"arrival_date_day_of_month\", \"stays_in_weekend_nights\",\n",
        "    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n",
        "    \"is_repeated_guest\", \"previous_cancellations\",\n",
        "    \"previous_bookings_not_canceled\", \"required_car_parking_spaces\",\n",
        "    \"total_of_special_requests\", \"adr\",\n",
        "]\n",
        "features_cat = [\n",
        "    \"hotel\", \"arrival_date_month\", \"meal\",\n",
        "    \"market_segment\", \"distribution_channel\",\n",
        "    \"reserved_room_type\", \"deposit_type\", \"customer_type\",\n",
        "]\n",
        "\n",
        "transformer_num = make_pipeline(\n",
        "    SimpleImputer(strategy=\"constant\"), # there are a few missing values\n",
        "    StandardScaler(),\n",
        ")\n",
        "transformer_cat = make_pipeline(\n",
        "    SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n",
        "    OneHotEncoder(handle_unknown='ignore'),\n",
        ")\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (transformer_num, features_num),\n",
        "    (transformer_cat, features_cat),\n",
        ")\n",
        "\n",
        "# stratify - make sure classes are evenlly represented across splits\n",
        "X_train, X_valid, y_train, y_valid = \\\n",
        "    train_test_split(X, y, stratify=y, train_size=0.75)\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_valid = preprocessor.transform(X_valid)\n",
        "\n",
        "input_shape = [X_train.shape[1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1) Define Model #\n",
        "\n",
        "The model we'll use this time will have both batch normalization and dropout layers. To ease reading we've broken the diagram into blocks, but you can define it layer by layer as usual.\n",
        "\n",
        "Define a model with an architecture given by this diagram:\n",
        "\n",
        "<figure style=\"padding: 1em;\">\n",
        "<img src=\"Architecture.png\" width=\"400\" alt=\"Diagram of network architecture: BatchNorm, Dense, BatchNorm, Dropout, Dense, BatchNorm, Dropout, Dense.\">\n",
        "<figcaption style=\"textalign: center; font-style: italic\"><center>Diagram of a binary classifier.</center></figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# YOUR CODE HERE: define the model given in the diagram\n",
        "model = ____"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()\n",
        "# The Required Output is \n",
        "#Model: \"sequential\"\n",
        "#┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
        "#┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
        "#┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
        "#│ batch_normalization             │ (None, 62)             │           248 │\n",
        "#│ (BatchNormalization)            │                        │               │\n",
        "#├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "#│ dense (Dense)                   │ (None, 256)            │        16,128 │\n",
        "#├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "#│ batch_normalization_1           │ (None, 256)            │         1,024 │\n",
        "#│ (BatchNormalization)            │                        │               │\n",
        "#├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "#│ dropout (Dropout)               │ (None, 256)            │             0 │\n",
        "#├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "#│ dense_1 (Dense)                 │ (None, 256)            │        65,792 │\n",
        "#├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "#│ batch_normalization_2           │ (None, 256)            │         1,024 │\n",
        "#│ (BatchNormalization)            │                        │               │\n",
        "#├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "#│ dropout_1 (Dropout)             │ (None, 256)            │             0 │\n",
        "#├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "#│ dense_2 (Dense)                 │ (None, 1)              │           257 │\n",
        "#└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
        " #Total params: 251,125 (980.96 KB)\n",
        " #Trainable params: 83,325 (325.49 KB)\n",
        "# Non-trainable params: 1,148 (4.48 KB)\n",
        "# Optimizer params: 166,652 (650.99 KB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2) Add Optimizer, Loss, and Metric #\n",
        "\n",
        "Now compile the model with the Adam optimizer and binary versions of the cross-entropy loss and accuracy metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    patience=5,\n",
        "    min_delta=0.001,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=512,\n",
        "    epochs=200,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
        "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
